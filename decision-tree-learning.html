<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta http-equiv="Cache-Control" content="no-siteapp">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1, minimum-scale=1, maximum-scale=1">
<meta name="renderer" content="webkit">
<meta name="google" value="notranslate">
<meta name="robots" content="index,follow">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Feily Zhang">
<meta name="twitter:description" content="">
<meta name="twitter:image:src" content="https://doc.feily.techimages/my.jpg">

<meta property="og:url" content="https://doc.feily.tech">
<meta property="og:title" content="Feily Zhang">
<meta property="og:description" content="">
<meta property="og:site_name" content="Feily Zhang">
<meta property="og:image" content="https://doc.feily.techimages/my.jpg">
<meta property="og:type" content="website">
<meta name="robots" content="noodp">

<meta itemprop="name" content="Feily Zhang">
<meta itemprop="description" content="">
<meta itemprop="image" content="https://doc.feily.techimages/my.jpg">

<link rel="canonical" href="https://doc.feily.tech">

<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-itouch-icon" href="/favicon.ico">
<link rel="stylesheet" href="/bundle/index.css">
<script type="text/javascript">
    var timeSinceLang = {
        year: '年前',
        month: '个月前',
        day: '天前',
        hour: '小时前',
        minute: '分钟前',
        second: '秒前'
    };
    var root = '';
</script>

<script type="text/javascript">
    var conn, reloadTimer, connectTimer;
    var connect = function() {
        conn = new WebSocket('ws://' + location.host + '/live');
        conn.onmessage = function(event) {
            if (event.data === 'change') {
                if (reloadTimer) clearTimeout(reloadTimer);
                reloadTimer = setTimeout(function() {
                    window.location.reload();
                }, 200);
            }
        };
        conn.onclose = function() {
            if (connectTimer) clearTimeout(connectTimer);
            connectTimer = setTimeout(function() {
                connect();
            }, 1000);
        };
    };
    connect();
</script>


        <meta name="keywords" content="Machine Learning,">
        <meta name="description" content="分而治之——决策树学习算法">
        <meta name="author" content="林奕清">
        <title>分而治之——决策树学习算法</title>
    </head>
    <body>
        <article class="container">
            <header class="header-wrap" style="font-family:NSimSun">
  <a class="index" href="/">
    <img class="logo" src="images/my.jpg" />
    Feily Zhang
  </a>
  <ul class="menu">
      <li class="menu-item"><a href="/archive.html">归档</a></li>
      <li class="menu-item"><a href="/tag.html">标签</a></li>
      <li class="menu-item"><a href="/atom.xml">订阅</a></li>
  </ul>
</header>

            <article class="main article">
                <h1 class="title" style="font-family:KaiTi">分而治之——决策树学习算法</h1>
                <section class="info" style="font-family:KaiTi">
                    <span class="avatar" style="background-image: url(images/my.jpg);"></span>
                    <a class="name" href="/about.me.html">林奕清</a>
                    
                    <span class="date" data-time="1553327725"><span class="from"></span></span>
                    
                    <span class="tags"><a class="tag" href="/tag/Machine%20Learning/index.html">Machine Learning</a></span>
                </section>
                <article class="content" style="font-family:KaiTi"><h2>一、简介</h2>

<p>决策树学习算法以树形结构建立模型，类似于流程图，该模型本身包含一系列逻辑决策。几个概念如下</p>

<ul>
<li>决策节点：根据某一特征(属性)作出决定；</li>
<li>决策分支：从决策节点引出的分支表示做出的选择；</li>
<li>终端节点(叶节点)：决策树由叶节点终止，叶节点表示遵循决策组合的结果。</li>
</ul>

<p>其运行过程为：数据的分类从根节点开始(根节点是第一个决策节点)，根据特征值遍历树上的各个决策节点。数据采用的是一个漏斗形的路径，它将每一条记录汇集到一个节点，在叶节点为该记录分配一个预测类。</p>

<h2>二、理论原理</h2>

<p>决策树是决策函数的可视化描述，决策函数是由训练集的特征空间训练出来的，不同属性的不同程度上的不同组合会形成不同的候选函数(或决策树)，拟合优度因函数(或者不同特征的不同程度上的不同组合)而异，有的明显性能出色，有的明显性能低劣，我们不可能将所有的候选函数全部生成然后一一测试，因为多数情况下随着特征的增加，候选函数呈指数级增长。我们的目的就是找到最优的候选函数，何为最优？浅显易懂的来说就是</p>

<blockquote>
<p>通过较少的测试达到正确分类，即树中所有路径都较短，整棵树较浅。</p>
</blockquote>

<p>要做到路径较短，那么也就意味着每一次的分裂都要是高效的，即每次分裂都可以提高子集的同质性或者纯度，这样减少了不必要的分割，整棵树必然会变浅。那么专业的来说就是</p>

<blockquote>
<p>每一次决策节点的分裂，得到的子集中的预测类的水平的比例与父集合中的预测类的水平的比例<strong>最大程度上</strong>更低。</p>
</blockquote>

<h2>三、数学定义</h2>

<p>决策树算法对以上<strong>最大程度上</strong>的定义是通过<strong>信息增益</strong>实现的。要解释信息增益，先要解释<strong>熵</strong>与<strong>条件熵</strong>。</p>

<p>熵：熵是随机变量的不确定性度量，信息的获取对应于熵的减少。熵越大说明信息量越大该事件内部越不同质，不确定性越大。熵越小说明信息量越小该事件内部越同质，不确定性越小。举个例子</p>

<ul>
<li>明天早上太阳必然升起。这个事件，即随机变量或该事件没有不确定性，是完全同质的(因为只有这一种可能，还是必然的)；</li>
<li>抛一枚硬币，出现正反面朝上的概率皆为0.5。这个事件的熵为1，即随机变量或该事件具有不确定性，是不同质的(因为样本空间存在两种可能，而不是一种)。</li>
</ul>

<p>所以，我们可以通过事件的熵来定义该事件内部(样本空间)的同质性或混乱性。熵越大，说明该事件样本空间越不同质越杂乱，实验结果的不确定性更大；熵越小，说明事件样本空间越同质越不杂乱，实验的不确定性更小。其计算公式为
<img src="data:image/gif;base64,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" data-src="/images/article/shang.png" alt="熵计算公式" /></p>

<p>条件熵：条件熵表示在已知随机变量X的条件下，随机变量Y的不确定性。将其定义为给定X的条件下Y的条件概率分布的熵对X的数学期望。其本质仍然是熵。其计算公式为
<img src="data:image/gif;base64,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" data-src="/images/article/tiaojianshang.png" alt="条件熵计算公式" /></p>

<p>在决策树算法当中，使用熵值来计算由每一个可能特征的分割所引起的同质性(均匀性)变化，该计算称为信息增益。对于特征F，信息增益的计算方法是分割前的数据分区(S_1)的熵值减去由分割产生的数据分区(S_2)的熵值。其中S_1就是分裂前集合预测类水平的熵值，但是集合被通过该特征划分为了若干个子集，怎样计算它们的熵值呢？</p>

<p>由于一次分割后，数据被划分到了多个分区中，因此计算S_2需要考虑所有分区的熵值总和，这并不意味着简单加总，而是求他们的期望，也就是在分割集合的条件下子集的熵对该次分割的数学期望，即S_2是分割后产生的若干个子集的预测类水平的熵值的期望(即从一个分割得到的总熵就是根据案例落入分区的比例加权的n个分区的熵值的总和)，这样就可以通过条件熵的方式来度量由分割产生的效果，即总熵减少到了多少(因为分割为了多个子集，所以总熵只能通过期望来计算)。这是一种度量方式，但是不够直观，最直观的度量方式就是计算信息增益，如下
<img src="data:image/gif;base64,R0lGODlhGAAYAPQAAP///wAAAM7Ozvr6+uDg4LCwsOjo6I6OjsjIyJycnNjY2KioqMDAwPLy8nd3d4aGhri4uGlpaQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAkHAAAAIf4aQ3JlYXRlZCB3aXRoIGFqYXhsb2FkLmluZm8AIf8LTkVUU0NBUEUyLjADAQAAACwAAAAAGAAYAAAFriAgjiQAQWVaDgr5POSgkoTDjFE0NoQ8iw8HQZQTDQjDn4jhSABhAAOhoTqSDg7qSUQwxEaEwwFhXHhHgzOA1xshxAnfTzotGRaHglJqkJcaVEqCgyoCBQkJBQKDDXQGDYaIioyOgYSXA36XIgYMBWRzXZoKBQUMmil0lgalLSIClgBpO0g+s26nUWddXyoEDIsACq5SsTMMDIECwUdJPw0Mzsu0qHYkw72bBmozIQAh+QQJBwAAACwAAAAAGAAYAAAFsCAgjiTAMGVaDgR5HKQwqKNxIKPjjFCk0KNXC6ATKSI7oAhxWIhezwhENTCQEoeGCdWIPEgzESGxEIgGBWstEW4QCGGAIJEoxGmGt5ZkgCRQQHkGd2CESoeIIwoMBQUMP4cNeQQGDYuNj4iSb5WJnmeGng0CDGaBlIQEJziHk3sABidDAHBgagButSKvAAoyuHuUYHgCkAZqebw0AgLBQyyzNKO3byNuoSS8x8OfwIchACH5BAkHAAAALAAAAAAYABgAAAW4ICCOJIAgZVoOBJkkpDKoo5EI43GMjNPSokXCINKJCI4HcCRIQEQvqIOhGhBHhUTDhGo4diOZyFAoKEQDxra2mAEgjghOpCgz3LTBIxJ5kgwMBShACREHZ1V4Kg1rS44pBAgMDAg/Sw0GBAQGDZGTlY+YmpyPpSQDiqYiDQoCliqZBqkGAgKIS5kEjQ21VwCyp76dBHiNvz+MR74AqSOdVwbQuo+abppo10ssjdkAnc0rf8vgl8YqIQAh+QQJBwAAACwAAAAAGAAYAAAFrCAgjiQgCGVaDgZZFCQxqKNRKGOSjMjR0qLXTyciHA7AkaLACMIAiwOC1iAxCrMToHHYjWQiA4NBEA0Q1RpWxHg4cMXxNDk4OBxNUkPAQAEXDgllKgMzQA1pSYopBgonCj9JEA8REQ8QjY+RQJOVl4ugoYssBJuMpYYjDQSliwasiQOwNakALKqsqbWvIohFm7V6rQAGP6+JQLlFg7KDQLKJrLjBKbvAor3IKiEAIfkECQcAAAAsAAAAABgAGAAABbUgII4koChlmhokw5DEoI4NQ4xFMQoJO4uuhignMiQWvxGBIQC+AJBEUyUcIRiyE6CR0CllW4HABxBURTUw4nC4FcWo5CDBRpQaCoF7VjgsyCUDYDMNZ0mHdwYEBAaGMwwHDg4HDA2KjI4qkJKUiJ6faJkiA4qAKQkRB3E0i6YpAw8RERAjA4tnBoMApCMQDhFTuySKoSKMJAq6rD4GzASiJYtgi6PUcs9Kew0xh7rNJMqIhYchACH5BAkHAAAALAAAAAAYABgAAAW0ICCOJEAQZZo2JIKQxqCOjWCMDDMqxT2LAgELkBMZCoXfyCBQiFwiRsGpku0EshNgUNAtrYPT0GQVNRBWwSKBMp98P24iISgNDAS4ipGA6JUpA2WAhDR4eWM/CAkHBwkIDYcGiTOLjY+FmZkNlCN3eUoLDmwlDW+AAwcODl5bYl8wCVYMDw5UWzBtnAANEQ8kBIM0oAAGPgcREIQnVloAChEOqARjzgAQEbczg8YkWJq8nSUhACH5BAkHAAAALAAAAAAYABgAAAWtICCOJGAYZZoOpKKQqDoORDMKwkgwtiwSBBYAJ2owGL5RgxBziQQMgkwoMkhNqAEDARPSaiMDFdDIiRSFQowMXE8Z6RdpYHWnEAWGPVkajPmARVZMPUkCBQkJBQINgwaFPoeJi4GVlQ2Qc3VJBQcLV0ptfAMJBwdcIl+FYjALQgimoGNWIhAQZA4HXSpLMQ8PIgkOSHxAQhERPw7ASTSFyCMMDqBTJL8tf3y2fCEAIfkECQcAAAAsAAAAABgAGAAABa8gII4k0DRlmg6kYZCoOg5EDBDEaAi2jLO3nEkgkMEIL4BLpBAkVy3hCTAQKGAznM0AFNFGBAbj2cA9jQixcGZAGgECBu/9HnTp+FGjjezJFAwFBQwKe2Z+KoCChHmNjVMqA21nKQwJEJRlbnUFCQlFXlpeCWcGBUACCwlrdw8RKGImBwktdyMQEQciB7oACwcIeA4RVwAODiIGvHQKERAjxyMIB5QlVSTLYLZ0sW8hACH5BAkHAAAALAAAAAAYABgAAAW0ICCOJNA0ZZoOpGGQrDoOBCoSxNgQsQzgMZyIlvOJdi+AS2SoyXrK4umWPM5wNiV0UDUIBNkdoepTfMkA7thIECiyRtUAGq8fm2O4jIBgMBA1eAZ6Knx+gHaJR4QwdCMKBxEJRggFDGgQEREPjjAMBQUKIwIRDhBDC2QNDDEKoEkDoiMHDigICGkJBS2dDA6TAAnAEAkCdQ8ORQcHTAkLcQQODLPMIgIJaCWxJMIkPIoAt3EhACH5BAkHAAAALAAAAAAYABgAAAWtICCOJNA0ZZoOpGGQrDoOBCoSxNgQsQzgMZyIlvOJdi+AS2SoyXrK4umWHM5wNiV0UN3xdLiqr+mENcWpM9TIbrsBkEck8oC0DQqBQGGIz+t3eXtob0ZTPgNrIwQJDgtGAgwCWSIMDg4HiiUIDAxFAAoODwxDBWINCEGdSTQkCQcoegADBaQ6MggHjwAFBZUFCm0HB0kJCUy9bAYHCCPGIwqmRq0jySMGmj6yRiEAIfkECQcAAAAsAAAAABgAGAAABbIgII4k0DRlmg6kYZCsOg4EKhLE2BCxDOAxnIiW84l2L4BLZKipBopW8XRLDkeCiAMyMvQAA+uON4JEIo+vqukkKQ6RhLHplVGN+LyKcXA4Dgx5DWwGDXx+gIKENnqNdzIDaiMECwcFRgQCCowiCAcHCZIlCgICVgSfCEMMnA0CXaU2YSQFoQAKUQMMqjoyAglcAAyBAAIMRUYLCUkFlybDeAYJryLNk6xGNCTQXY0juHghACH5BAkHAAAALAAAAAAYABgAAAWzICCOJNA0ZVoOAmkY5KCSSgSNBDE2hDyLjohClBMNij8RJHIQvZwEVOpIekRQJyJs5AMoHA+GMbE1lnm9EcPhOHRnhpwUl3AsknHDm5RN+v8qCAkHBwkIfw1xBAYNgoSGiIqMgJQifZUjBhAJYj95ewIJCQV7KYpzBAkLLQADCHOtOpY5PgNlAAykAEUsQ1wzCgWdCIdeArczBQVbDJ0NAqyeBb64nQAGArBTt8R8mLuyPyEAOw==" data-src="/images/article/information-gain.png" alt="信息增益计算公式" /></p>

<p>由于子集有且只有一个父集(可能会共享同一个父集，但是绝不会同时拥有两个或两个以上的直接父集)，那么也就意味着分割前的熵是相同的，那么分割后的子集的条件熵越小就越说明当前训练集在该选择该特征进行划分后效果最好，因为分割后的总熵(期望熵)越小，即内部越同质越均匀纯度越高。为了便于与其余的特征进行比较，通过计算信息增益，那么也就意味着对同一训练集根据不同特征进行分裂，哪个特征的信息增益越大那么划分的结果越好越同质越均匀越纯。</p>

<p>到此为止，可以看到，决策树算法是通过<strong>信息增益</strong>来确定合理的或最好的分割点的，信息增益在此程度上意味着一次分割获得的信息多少，信息增益越大，那么得到的信息量越多，那么分割的越成功，分割的次数越少，最终树中的所有路径越短，整棵树越浅。</p>

<p>简言之：</p>

<p><strong>熵越小越说明当前训练集(或子集)内部纯度越高越同质，这是我们的最终目标。刚开始，熵肯定是很大的，我们的目的就是通过根据合适的特征的分裂降低子集的熵值以实现分类预测，而合适的特征的选择是通过信息增益来衡量的。</strong></p>

<h2>四、过度拟合与剪枝</h2>

<p>过度拟合的专业定义显得过于繁琐和难懂，不妨使用《Artificial Intelligence: A Modern Approach》一书中关于过度拟合的形象化描述，如下</p>

<blockquote>
<p>Consider the problem of trying to predict whether the roll of a die will come up as 6 or not Suppose that experiments are carried out with various dice and that the attributes describing each training example include the color of the die, its weight, the time when the roll was done, and whether the experimenters had their fingers crossed. If the dice are fair, the right thing to learn is a tree with a single node that says &ldquo;no,&rdquo; But the DECISION-TREE-LEARNING algorithm will seize on any pattern it can find in the input. If it turns out that there are 2 rolls of a 7-gram blue die with fingers crossed and they both come out 6, then the algorithm may construct a path that predicts 6 in that case. This problem is called overfitting.</p>
</blockquote>

<p>对决策树而言，一种被称为<strong>决策树剪枝</strong>的技术可以减轻过度拟合。包括两种，分别是<strong>预剪枝</strong>和<strong>后剪枝</strong>。</p>

<p>所谓<strong>预剪枝</strong>就是一旦决策树达到了一定数量的决策，或者说决策节点仅含有少量的案例，那么就停止树的生长，因为这样的继续生长是毫无意义的，决策节点只含有少量的案例是不具备一般性意义的。或者说当不存在可用于分裂的好属性时，就停止生长，这种情况就是允许该子集存在一定的不同质以防止过度拟合，因为再此基础上继续生长仍然不具备一般性意义，还是因为案例太少，不具备观测与分析价值。</p>

<p>预剪枝的好处就是及时停止树的生长，而不是等树生长之后再修剪；简单而直接，适用于解决大规模问题，因为数据量很大的情况下生成被后剪的不必要决策节点是毫无意义的。但是要精确的实现决策树的修剪并不容易，如果修剪的过早那么预测分类不同质的可能性更大，影响决策树的优度；而如果修剪的过晚，又会造成一定程度的过度拟合。</p>

<p>预剪枝的几种方法如下</p>

<ol>
<li>定义一个高度，当决策树达到该高度时就停止决策树的生长，这是最简单的一种办法；</li>
<li>达到某叶节点的案例(或记录)具有相同的特征向量，即使这些案例不属于同一类，那么也可以停止决策树的生长，因为既然当前节点的案例集具备相同的特征向量，那么就有很大的可能性认为其与当前预测类同质，这个度还是要把握好，如果停止的过早仍然会使得叶节点预测类不同质可能性变大；</li>
<li>定义一个阈值，当达到某个节点的案例个数小于该阈值时就可以停止该决策树的生长，原因仍然是少量样本不具备观测与分析价值，对其分析(选择合适的分割点生成决策节点)容易造成过度拟合；</li>
<li>定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长。</li>
</ol>

<p>所谓<strong>后剪枝</strong>就是先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的节点子树用叶节点来代替，该叶节点的预测类型为该节点子树中最频繁的类型。几种后剪枝方法如下</p>

<ol>
<li>Reduced-Error Pruning(REP, 错误率降低剪枝)</li>
<li>Pessimistic Error Pruning(PEP, 悲观错误剪枝)</li>
<li>Cost-Complexity Pruning(CCP, 代价复杂度剪枝)</li>
</ol>

<p>其中REP剪枝的原理与操作如下</p>

<p>该方法将可用数据集合分为两个样例集合，一个训练集用于训练决策树，一个验证集用来评估修剪这个决策树后得到的预测分类的结果的精度以决定是否修剪相应子树。其操作方法如下</p>

<ol>
<li>删除以此节点为根的子树；</li>
<li>使其成为叶子节点；</li>
<li>赋予该叶子节点关联的训练数据的最常见分类，即将原子树最最频繁的类型赋给该叶子节点；</li>
<li>当修建后的树对于验证集合的性能不会比原来的树差时，才真正删除该子树或者用叶节点替换该子树。</li>
</ol>

<p>由于使用独立的验证集，与原始决策树相比，修建后的决策树可能偏向于过度修剪。这是因为一些不会在验证集中出现的稀少的但是在训练集中出现的案例会被修剪，但是这并不意味着该案例在其后的测试集中不会在出现。如果训练集较小，不建议使用REP剪枝算法。</p>

<h2>五、R语言中C5.0决策树算法</h2>

<p>C5.0算法是C4.5的改进版，适用于大多数类型的问题。与其它机器学习模型相比，该算法建立的决策树一般都表现的与它们的模型几乎一模一样，且更容易理解与部署。该算法的优点之一就是它可以自动修剪，其总体策略就是后剪枝，它先生成一个过度拟合的决策树，然后删除对分类误差影响不大的节点和分枝。在某些情况下，整个分枝会被进一步向上移动或者被一些简单的决策所取代，这两个移植分枝的过程被称为子树提升和子树替换。C5.0算法的优点之一就是它很容易调整训练方案以提升决策树性能。</p>

<p>该算法在C50添加包当中，其函数原型为</p>

<p>先创建分类器</p>

<pre><code>m &lt;- C5.0(train, class, trials = 1, costs = NULL)
</code></pre>

<ul>
<li>train : 一个包含训练数据的数据框;</li>
<li>class : 包含训练数据每一行的分类的一个因子向量;</li>
<li>trials: 可选数值，用于控制自助法循环次数(默认为1)，可用于提升模型性能;</li>
<li>costs : 可选矩阵，用于给出与各种类型错误相对应的成本，可用于提升模型性能。</li>
</ul>

<p>该函数返回一个C5.0模型对象，该对象能够用于预测。</p>

<p>进行预测</p>

<pre><code>p &lt;- predict(m, test, type = &quot;class&quot;)
</code></pre>

<ul>
<li>m : 由C5.0函数得到的训练模型;</li>
<li>test : 一个包含测试数据的数据框，该数据框和用来创建分类器的训练数据有同样的特征;</li>
<li>type : 取值为class或者prob, 标识预测是最有可能的类别值或者原始的预测概率;</li>
</ul>

<p>该函数返回一个向量，对应test测试集的每一个案例的预测分类结果或概率值(由type决定)。</p>

<h2>六、R语言C5.0算法应用示例</h2>

<p>以《Machine Learning With R》一书中的根据银行贷款数据来建立模型来预测贷款是否违约为例来说明C5.0算法</p>

<pre><code>&gt; getwd()
[1] &quot;C:/Users/Administrator/Documents&quot;
&gt; setwd(&quot;C:\\Users\\Administrator\\Desktop\\docs\\Machine-Learning-with-R-datasets-master&quot;)
&gt; credit &lt;- read.csv(&quot;credit.csv&quot;)
&gt; str(credit$default)
 int [1:1000] 1 2 1 1 2 1 1 1 1 2 ...
&gt; table(credit$default)

  1   2 
700 300 
&gt; credit$default &lt;- factor(credit$default, levels = c(1, 2), labels = c(&quot;no&quot;, &quot;yes&quot;))
&gt; table(credit$default)

 no yes 
700 300 
&gt; set.seed(12345)
&gt; credit_rand &lt;- credit[order(runif(1000)), ]
&gt; credit_train &lt;- credit_rand[1 : 900, ]
&gt; credit_test &lt;- credit_rand[901 : 1000, ]
&gt; prop.table(table(credit_train$default))

       no       yes 
0.7022222 0.2977778 
&gt; prop.table(table(credit_test$default))

  no  yes 
0.68 0.32 
</code></pre>

<p>上述将数据框credit按照均匀分布的1000个随机数重组，最终为credit_rand，然后分别分割为训练集credit_train和测试集credit_test，通过校验其分布发现基本上与原数据框分布无异。
然后应用C5.0算法创建分类器</p>

<pre><code>&gt; library(C50)
Warning message:
程辑包‘C50’是用R版本3.5.3 来建造的 
&gt; credit_model &lt;- C5.0(credit_train[-17], credit_train$default)
</code></pre>

<p>我们可以通过输出分类器名称来查看关于决策树的一些基本数据</p>

<pre><code>&gt; credit_model

Call:
C5.0.default(x = credit_train[-17], y = credit_train$default)

Classification Tree
Number of samples: 900 
Number of predictors: 20 

Tree size: 57 

Non-standard options: attempt to group attributes
</code></pre>

<p>前面的文字反映了关于该决策树的一些基本情况，包括生成决策树的函数调用、特征数(predictors)、用于决策树增长的案例(Samples)。同时列出了树的大小是57，表明该决策树包含57个决策。
如果要查看决策树，可以通过<code>summary()</code>函数来实现，部分输出为</p>

<pre><code>&gt; summary(credit_model)

Call:
C5.0.default(x = credit_train[-17], y = credit_train$default)


C5.0 [Release 2.07 GPL Edition]     Sat Mar 23 20:33:44 2019
-------------------------------

Class specified by attribute `outcome'

Read 900 cases (21 attributes) from undefined.data

Decision tree:

checking_balance = unknown: no (358/44)
checking_balance in {&lt; 0 DM,&gt; 200 DM,1 - 200 DM}:
:...foreign_worker = no:
    :...installment_plan in {none,stores}: no (17/1)
    :   installment_plan = bank:
    :   :...residence_history &lt;= 3: yes (2)
    :       residence_history &gt; 3: no (2)
    foreign_worker = yes:


Evaluation on training data (900 cases):

        Decision Tree   
      ----------------  
      Size      Errors  

        57  127(14.1%)   &lt;&lt;


       (a)   (b)    &lt;-classified as
      ----  ----
       590    42    (a): class no
        85   183    (b): class yes


    Attribute usage:

    100.00% checking_balance
     60.22% foreign_worker
     57.89% credit_history
     51.11% months_loan_duration
     42.67% savings_balance
     30.44% other_debtors
     17.78% job
     15.56% installment_plan
     14.89% purpose
     12.89% employment_length
     10.22% amount
      6.78% residence_history
      5.78% housing
      3.89% dependents
      3.56% installment_rate
      3.44% personal_status
      2.78% age
      1.56% property
      1.33% existing_credits


Time: 0.0 secs
</code></pre>

<p>该输出包含一个混淆矩阵，是一个交叉列表，表示模型对训练数据错误分类的记录数。可以看出共有42个原本是no的案例被预测为yes，共有85个原本是yes的案例被预测为no。即该决策树对训练集中127个案例错误的分类了，占比为14.1%</p>

<p>现在有了分类器，我们可以用于预测，如下</p>

<pre><code>&gt; credit_prediction &lt;- predict(credit_model, credit_test)
</code></pre>

<p>预测的结果向量为</p>

<pre><code>&gt; credit_prediction
  [1] no  yes no  no  no  yes no  no  yes yes no  no  no  yes yes yes no  no  yes
 [20] no  yes no  yes no  yes no  yes no  no  no  yes yes no  yes no  no  no  no 
 [39] yes yes no  no  no  yes yes yes yes no  yes no  no  no  yes no  no  no  no 
 [58] yes no  no  no  yes no  yes no  no  yes no  no  no  no  no  no  yes yes no 
 [77] no  yes no  no  no  no  no  no  no  no  no  no  yes yes yes no  no  no  no 
 [96] yes no  yes no  no 
Levels: no yes
</code></pre>

<p>可以通过如下方法查看测试集的预测成功程度，以评估模型性能</p>

<pre><code>&gt; table(credit_prediction == credit_test$default)

FALSE  TRUE 
   25    75 

&gt; prop.table(table(credit_prediction == credit_test$default))

FALSE  TRUE 
 0.25  0.75 
</code></pre>

<p>可见，正确率为75%。</p>

<p>如何提高模型性能呢？</p>

<h2>七、提高C5.0算法的分类器性能</h2>

<p>之前在创建分类器的时候有两个参数没有使用，分别是trials和costs参数。</p>

<p>对C5.0算法性能的提升可以通过<strong>自适应增强(adaptive boosting)算法</strong>，具体做法是在决策树的构建过程中，这些决策树通过投票表决的方法为每个案例选择最佳的分类。C5.0算法通过trials参数来引入该算法(boosting算法), 该参数的值表示在模型增强团队中使用到的独立决策树的数量，参数trials设置了一个上限，如果该算法识别出额外的实验似乎并没有提高模型的准确性，那么它将停止添加决策树。一般设置为10</p>

<pre><code>&gt; credit_model &lt;- C5.0(credit_train[-17], credit_train$default, trials = 10)
&gt; credit_prediction &lt;- predict(credit_model, credit_test)
&gt; prop.table(table(credit_prediction == credit_test$default))

FALSE  TRUE 
 0.21  0.79 
</code></pre>

<p>可以看到明显提升了模型精度。</p>

<p>还有一个办法是通过costs参数，该参数是一个代价矩阵，用于指定每种错误相对于其它任何错误有多少倍的严重性。假如评估模型精度的双向交叉表为</p>

<pre><code>&gt; library(gmodels)
Warning message:
程辑包‘gmodels’是用R版本3.5.3 来建造的 
&gt; CrossTable(credit_test$default, credit_prediction, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&quot;actual default&quot;, &quot;predicted default&quot;))

 
   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  100 

 
               | predicted default 
actual default |        no |       yes | Row Total | 
---------------|-----------|-----------|-----------|
            no |        63 |         5 |        68 | 
               |     0.630 |     0.050 |           | 
---------------|-----------|-----------|-----------|
           yes |        16 |        16 |        32 | 
               |     0.160 |     0.160 |           | 
---------------|-----------|-----------|-----------|
  Column Total |        79 |        21 |       100 | 
---------------|-----------|-----------|-----------|

</code></pre>

<p>那么我们定义如下代价矩阵</p>

<pre><code>&gt; error_cost &lt;- matrix(c(0, 4, 1, 0), nrow = 2)
&gt; error_cost
     [,1] [,2]
[1,]    0    1
[2,]    4    0
</code></pre>

<p>该代价矩阵用来说明双向交叉表中落入对应[2, 1]的元素的错误更加严重，那么分类器就会极力避免这种情况发生，再创建一个分类器试试</p>

<pre><code>&gt; credit_model &lt;- C5.0(credit_train[-17], credit_train$default, costs = error_cost)
Warning message:
no dimnames were given for the cost matrix; the factor levels will be used 
&gt; credit_prediction &lt;- predict(credit_model, credit_test)
&gt; prop.table(table(credit_prediction == credit_test$default))

FALSE  TRUE 
 0.32  0.68 
&gt; CrossTable(credit_test$default, credit_prediction, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&quot;actual default&quot;, &quot;predicted default&quot;))

 
   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  100 

 
                    | credit_prediction 
credit_test$default |        no | Row Total | 
--------------------|-----------|-----------|
                 no |        68 |        68 | 
                    |     0.680 |           | 
--------------------|-----------|-----------|
                yes |        32 |        32 | 
                    |     0.320 |           | 
--------------------|-----------|-----------|
       Column Total |       100 |       100 | 
--------------------|-----------|-----------|
</code></pre>

<p>可见，与之前的模型相比，该模型的精度更低。但是这不失为一种调整模型性能的方法，需要根据实际情况来调整。</p>
</article>
                <section class="author" style="font-family:NSimSun">
                    <div class="avatar" style="background-image: url(images/my.jpg);"></div>
                    <a class="name" href="/about.me.html">林奕清</a>
                    <div class="intro">但远山长，云山乱，晓山青</div>
                </section>
                <section class="recommend" style="font-family:KaiTi">
                    
                    <section class="nav prev more">
                        <div class="head">上篇文章</div>
                        <a class="link" href="/artificial-neural-network.html">黑箱方法——人工神经网络</a>
                    </section>
                    
                    
                    <section class="nav next more">
                        <div class="head">下篇文章</div>
                        <a class="link" href="/analysis-of-arrays-classes.html">Arrays类的相关用法</a>
                    </section>
                    
                </section>
                
            </article>
        </article>
        <footer class="footer" style="font-family:NSimSun">
    <span class="copyright">
        Feily Zhang ©
        <script type="text/javascript">
            document.write(new Date().getFullYear());
        </script>
    </span>
    <span class="publish">Powered by <a href="http://www.chole.io/" target="_blank">Ink</a></span>
</footer>

        <script src="/bundle/index.js"></script>
    </body>
</html>
